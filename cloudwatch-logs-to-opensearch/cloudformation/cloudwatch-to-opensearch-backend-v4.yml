AWSTemplateFormatVersion: 2010-09-09
Metadata:
  TemplateId: "cloudwatch-to-opensearch-backend"
Parameters:
  OSSADMIN:
    Type: "String"
    Default: "CHANGEME"
    Description: "The role to use for admin access to OpenSearch Serverless Collection."
Resources:
  SQSQueuePolicyS3Trigger:
    Type: 'AWS::SQS::QueuePolicy'
    Properties:
      Queues:
        - !Ref SQSQueueS3Trigger
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: 's3.amazonaws.com'
            Action: 'sqs:SendMessage'
            Resource: !GetAtt SQSQueueS3Trigger.Arn

  SQSQueueS3Trigger:
    Type: "AWS::SQS::Queue"
    Properties:
      SqsManagedSseEnabled: true
      ReceiveMessageWaitTimeSeconds: 0
      DelaySeconds: 0
      MessageRetentionPeriod: 345600
      MaximumMessageSize: 262144
      VisibilityTimeout: 30
      QueueName: "cloudwatch-ingest-s3-queue"
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"

  S3BucketLogs:
    Type: "AWS::S3::Bucket"
    DependsOn:  
      - SQSQueuePolicyS3Trigger
      - SQSQueueS3Trigger
    Properties:
      NotificationConfiguration:
        QueueConfigurations:
          - Event: "s3:ObjectCreated:*"
            Queue: !GetAtt SQSQueueS3Trigger.Arn
      BucketName:  !Sub
          - 'cloudwatch-logs-to-osis-${RandomGUID}'
          - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"


  KinesisFirehoseDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: "cloudwatch-to-s3"
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt S3BucketLogs.Arn
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 5
        CompressionFormat: UNCOMPRESSED
        Prefix: ""
        RoleARN: !GetAtt IAMRoleKinesisFirehoseServiceRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt LambdaFunctionKDFopensearchServerlessTransform.Arn
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"

  IAMRoleKinesisFirehoseServiceRole:
    Type: "AWS::IAM::Role"
    Properties:
      Path: "/service-role/"
      ManagedPolicyArns:
      - Ref: "IAMManagedPolicyKinesisFirehoseServicePolicy"
      MaxSessionDuration: 3600
      RoleName: !Sub 
          - 'KinesisFirehoseServiceRole-cloudwatch-to-opensearch-${RandomGUID}'
          - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "firehose.amazonaws.com"
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"

  IAMManagedPolicyKinesisFirehoseServicePolicy:
    Type: "AWS::IAM::ManagedPolicy"
    Properties:
      ManagedPolicyName: !Sub 
        - 'KDFManagedPolicy-cloudwatch-to-s3-${RandomGUID}'
        -  { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      Path: "/service-role/"
      Description: ""
      Groups: []
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Join ['', [ "arn:aws:glue:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":catalog" ]]
          - !Join ['', [ "arn:aws:glue:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":database/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          - !Join ['', [ "arn:aws:glue:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":database/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          Action:
          - "glue:GetTable"
          - "glue:GetTableVersion"
          - "glue:GetTableVersions"
          Effect: "Allow"
          Sid: ""
        - Resource:
          - !Join ['', [ "arn:aws:glue:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":registry/*" ]]
          - !Join ['', [ "arn:aws:glue:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":schema/*" ]]
          Action:
          - "glue:GetSchemaByDefinition"
          Effect: "Allow"
          Sid: ""
        - Resource:
          - "*"
          Action:
          - "glue:GetSchemaVersion"
          Effect: "Allow"
          Sid: ""
        - Resource:
          - !Join [ '', [ 'arn:aws:s3:::', {"Ref": "S3BucketLogs"} ]]
          - !Join [ '', [ 'arn:aws:s3:::', {"Ref": "S3BucketLogs"}, '/*' ]]
          Action:
          - "s3:AbortMultipartUpload"
          - "s3:GetBucketLocation"
          - "s3:GetObject"
          - "s3:ListBucket"
          - "s3:ListBucketMultipartUploads"
          - "s3:PutObject"
          Effect: "Allow"
          Sid: ""
        - Resource: !GetAtt LambdaFunctionKDFopensearchServerlessTransform.Arn
          Action:
          - "lambda:InvokeFunction"
          - "lambda:GetFunctionConfiguration"
          Effect: "Allow"
          Sid: ""
        - Condition:
            StringEquals:
              kms:ViaService: "s3.${AWS::Region}.amazonaws.com"
            StringLike:
              kms:EncryptionContext:aws:s3:arn:
              - "arn:aws:s3:::%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%/*"
              - "arn:aws:s3:::%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%"
          Resource:
          - !Join ['', [ "arn:aws:kms:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":key/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          Action:
          - "kms:GenerateDataKey"
          - "kms:Decrypt"
          Effect: "Allow"
        - Resource:
          - !Join ['', [ "arn:aws:logs:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":log-group:/aws/kinesisfirehose/cloudwatch-to-s3:log-stream:*" ]]
          Action:
          - "logs:PutLogEvents"
          Effect: "Allow"
          Sid: ""
        - Resource: !Join ['', [ "arn:aws:kinesis:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":stream/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          Action:
          - "kinesis:DescribeStream"
          - "kinesis:GetShardIterator"
          - "kinesis:GetRecords"
          - "kinesis:ListShards"
          Effect: "Allow"
          Sid: ""
        - Condition:
            StringEquals:
              kms:ViaService: !Join ['', [ "kinesis.", {"Ref": "AWS::Region"},".amazonaws.com" ]]
            StringLike:
              kms:EncryptionContext:aws:kinesis:arn: !Join ['', [ "arn:aws:kinesis:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":stream/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          Resource:
          - !Join ['', [ "arn:aws:kms:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":key/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%" ]]
          Action:
          - "kms:Decrypt"
          Effect: "Allow"

  IAMRoleKDFOpenSearchServerlessTransformRole:
    Type: "AWS::IAM::Role"
    Properties:
      Path: "/service-role/"
      ManagedPolicyArns:
      - Ref: "IAMManagedPolicyServiceRoleAWSLambdaBasicExecutionRole"
      MaxSessionDuration: 3600
      RoleName: !Sub 
        - 'kdf-opensearch-serverless-transform-role-${RandomGUID}'
        -  { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"

  IAMRoleCloudWatchLogsToKinesisRole:
    Type: "AWS::IAM::Role"
    Properties:
      Path: "/"
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/AmazonKinesisFirehoseFullAccess"
      - "arn:aws:iam::aws:policy/AmazonKinesisFullAccess"
      MaxSessionDuration: 3600
      RoleName: !Sub
        - 'cloudWatch-subscription-to-kinesis-role-${RandomGUID}'
        -  { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      Description: "allow cloudwatch log subscriptions full access to kinesis streams and KDF"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Condition:
              StringLike:
                aws:SourceArn: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*"
            Action: "sts:AssumeRole"
            Effect: "Allow"
            Sid: "Statement1"
            Principal:
              Service: "logs.amazonaws.com"

  IAMManagedPolicyServiceRoleAWSLambdaBasicExecutionRole:
    Type: "AWS::IAM::ManagedPolicy"
    Properties:
      ManagedPolicyName:  !Sub 
        - 'KDFOpenSearchServerlessTransform-Managed-Policy-${RandomGUID}'
        -  { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      Path: "/service-role/"
      Description: "Managed policy for KDF OpenSearch Tranform Lambda"
      Groups: []
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: 
          - !Join ['', [ "arn:aws:logs:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":*" ]]
          Action: "logs:CreateLogGroup"
          Effect: "Allow"
        - Resource:
          - !Join ['', [ "arn:aws:logs:", {"Ref": "AWS::Region"},":", {"Ref": "AWS::AccountId"},":*" ]]
          Action:
          - "logs:CreateLogStream"
          - "logs:PutLogEvents"
          Effect: "Allow"

  OSSDataAccessPolicy:
    Type: AWS::OpenSearchServerless::AccessPolicy 
    Properties:
      Name: oss-cw-data-access-policy
      Type: data
      Description: Data access policy for my collection
      Policy:  !Sub "[{\"Description\":\"Access for admin user\",\"Rules\":[{\"ResourceType\":\"index\",\"Resource\":[\"index/*/*\"],\"Permission\":[\"aoss:*\"]}, \
        \  {\"ResourceType\":\"collection\",\"Resource\":[\"collection/oss-cloudwatch\"],\"Permission\":[\"aoss:*\"]}], \
        \ \"Principal\":[\"arn:aws:iam::${AWS::AccountId}:role/${OSSADMIN}\", \"arn:aws:iam::${AWS::AccountId}:role/${OSSPipelineRole}\"]}]"

  OSSNetworkPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: oss-cloudwatch-network-policy
      Type: network
      Description: Network policy for quickstart collection
      Policy: >-
        [{"Rules":[{"ResourceType":"collection","Resource":["collection/oss-cloudwatch"]}, {"ResourceType":"dashboard","Resource":["collection/oss-cloudwatch"]}],"AllowFromPublic":true}]

  OSSEncryptionPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: oss-cloudwatch-security-policy
      Type: encryption
      Description: Encryption policy for oss-cloudwatch collection
      Policy: >-
        {"Rules":[{"ResourceType":"collection","Resource":["collection/oss-cloudwatch"]}],"AWSOwnedKey":true}

  OSSCollection:
    Type: AWS::OpenSearchServerless::Collection
    Properties:
      Type: "SEARCH"
      Description: "The opensearch collection where cloudwatch logs will be aggreagated for search"
      Name: "oss-cloudwatch"
    DependsOn: 
      - OSSEncryptionPolicy
      - OSSNetworkPolicy

  OSSPipelineRole:
    Type: "AWS::IAM::Role"
    Properties:
      Path: "/"
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/AmazonSQSFullAccess"
      - "arn:aws:iam::aws:policy/AmazonS3FullAccess"
      MaxSessionDuration: 3600
      RoleName: "OSSPipelineRole"
      Description: "Allows OpenSearch Ingestion pipelines to call other AWS services\
        \ on your behalf."
      Policies:
        - PolicyName: "policy1"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              Resource: !Sub
                - "arn:aws:s3:::${BUCKETNAME}/*"
                - BUCKETNAME: !Ref S3BucketLogs 
              Action: "s3:GetObject"
              Effect: "Allow"
              Sid: "ReadFromS3"
        - PolicyName: "policy2"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              Resource: !GetAtt SQSQueueS3Trigger.Arn
              Action:
                - "sqs:DeleteMessage"
                - "sqs:ReceiveMessage"
              Effect: "Allow"
              Sid: "ReceiveAndDeleteSqsMessages"
        - PolicyName: "policy3"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              Resource: !Sub "arn:aws:aoss:${AWS::Region}:${AWS::AccountId}:collection/*"
              Action:
                - "aoss:APIAccessAll"
                - "aoss:BatchGetCollection"
              Effect: "Allow"
              Sid: "AOSSPermissions"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "osis-pipelines.amazonaws.com"

  LogGroupOSIS:
    Type: "AWS::Logs::LogGroup"
    Properties:
      LogGroupClass: "STANDARD"
      LogGroupName: "/aws/vendedlogs/OpenSearchIngestion/cloudwatch-s3-pipeline/audit-logs"
      DataProtectionPolicy: {}

  LogGroupKDFLambda:
    Type: "AWS::Logs::LogGroup"
    Properties:
      LogGroupClass: "STANDARD"
      LogGroupName: "/aws/lambda/kdf-opensearch-serverless-transform"
      DataProtectionPolicy: {}

  OSISPipeline:
    Type: "AWS::OSIS::Pipeline"
    Properties:
      PipelineConfigurationBody: !Sub |
         version: "2"  
         s3-log-pipeline: 
           source: 
              s3: 
                # Prevent data loss by only considering logs to be processed successfully after they are received by the opensearch sink 
                acknowledgments: true 
                notification_type: "sqs" 
                 # Provide compression property, can be "none", "gzip", or "automatic" 
                compression: "none" 
                codec: 
                  json: 
                sqs: 
                  # Provide a SQS Queue URL to read from 
                  queue_url: "${SQSQueueS3Trigger.QueueUrl}" 
                  # Lower maximum_messages depending on the size of your S3 objects 
                  maximum_messages: 10 
                  # Modify the visibility_timeout of the sqs messages depending on the size of your access log S3 objects. 
                  # Objects that are small (< 1.5 GB) and evenly distributed in size will result in the best performance 
                  # It is recommended to allocate a minimum of 30 seconds, and to add 30 seconds for every 0.25 GB of data in each S3 Object 
                  visibility_timeout: "30s" 
                aws: 
                  # Provide the region to use for aws credentials 
                  region: "${AWS::Region}" 
                  # Provide the role to assume for requests to SQS and S3 
                  sts_role_arn: "${OSSPipelineRole.Arn}" 
           processor: 
              - date: 
                  from_time_received: true 
                  destination: "@timestamp" 
           sink: 
              - opensearch: 
                  # Provide an AWS OpenSearch Service domain endpoint 
                  hosts: [ "${OSSCollection.CollectionEndpoint}" ] 
                  aws: 
                      # Provide a Role ARN with access to the domain. This role should have a trust relationship with osis-pipelines.amazonaws.com 
                      sts_role_arn: "${OSSPipelineRole.Arn}" 
                      # Provide the region of the domain. 
                      region: "${AWS::Region}" 
                      # Enable the 'serverless' flag if the sink is an Amazon OpenSearch Serverless collection 
                      serverless: true 
                  index: "cloudwatch_logs" 
                  # Enable the S3 DLQ to capture any failed requests in an S3 bucket 
                  #dlq: 
                  #   s3: 
                  #    # Provide an S3 bucket 
                  #      bucket: "skyjason-testing" 
                  #    # Provide a key path prefix for the failed requests 
                  #      key_path_prefix: "log-pipeline/logs/dlq" 
                  #    # Provide the region of the bucket. 
                  #      region: "us-east-1" 
                  #    # Provide a Role ARN with access to the bucket. This role should have a trust relationship with osis-pipelines.amazonaws.com 
                  #      sts_role_arn: "arn:aws:iam::${AWS::AccountId}:role/OSSPipelineRole2" 
      MinUnits: 1
      PipelineName: "cloudwatch-s3-pipeline"
      MaxUnits: 4
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"
      LogPublishingOptions:
        CloudWatchLogDestination:
          LogGroup: "/aws/vendedlogs/OpenSearchIngestion/cloudwatch-s3-pipeline/audit-logs"
        IsLoggingEnabled: true

  TestLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: "/aws/lambda/cloudwatch-to-opensearch"
      RetentionInDays: 7

  TestLogSubscription:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      DestinationArn: !GetAtt 'KinesisFirehoseDeliveryStream.Arn'
      FilterName: "ALL"
      FilterPattern: ""
      LogGroupName: !Ref 'TestLogGroup'
      RoleArn: !GetAtt 'IAMRoleCloudWatchLogsToKinesisRole.Arn'
      Distribution: ByLogStream
    DependsOn: 
      - KinesisFirehoseDeliveryStream
      - TestLogGroup


  LambdaFunctionKDFopensearchServerlessTransform:
    Type: "AWS::Lambda::Function"
    Properties:
      MemorySize: 128
      Description: "(FINAL) solution kinesis data firehose transform function to stream\
        \ to OpenSearch Serverless"
      TracingConfig:
        Mode: "PassThrough"
      Timeout: 30
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Handler: "index.lambda_handler"
      Role:
        Fn::GetAtt:
        - "IAMRoleKDFOpenSearchServerlessTransformRole"
        - "Arn"
      FileSystemConfigs: []
      FunctionName: "kdf-opensearch-serverless-transform"
      Runtime: "python3.11"
      PackageType: "Zip"
      LoggingConfig:
        LogFormat: "Text"
        LogGroup: "/aws/lambda/kdf-opensearch-serverless-transform"
      EphemeralStorage:
        Size: 512
      Tags:
        - Key: "source"
          Value: "aws-samples"  
        - Key: "solution"
          Value: "cloudwatch-to-opensearch"
      Architectures:
      - "x86_64"
      Code:
        ZipFile: |
          # Copyright 2023, Amazon.com, Inc. or its affiliates. All Rights Reserved.
          #
          # Licensed under the Amazon Software License (the "License").
          # You may not use this file except in compliance with the License.
          # A copy of the License is located at
          #
          #  http://aws.amazon.com/asl/
          #
          # or in the "license" file accompanying this file. This file is distributed
          # on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
          # express or implied. See the License for the specific language governing
          # permissions and limitations under the License.

          """
          For processing data sent to Firehose by Cloudwatch Logs subscription filters.

          Cloudwatch Logs sends to Firehose records that look like this where type is DATA_MESSAGE:

          {
            "messageType": "DATA_MESSAGE",
            "owner": "123456789012",
            "logGroup": "log_group_name",
            "logStream": "log_stream_name",
            "subscriptionFilters": [
              "subscription_filter_name"
            ],
            "logEvents": [
              {
                "id": "01234567890123456789012345678901234567890123456789012345",
                "timestamp": 1510109208016,
                "message": "log message 1"
              },
              {
                "id": "01234567890123456789012345678901234567890123456789012345",
                "timestamp": 1510109208017,
                "message": "log message 2"
              }
              ...
            ]
          }

          The "message" value is compressed with GZIP and base64 encoded.

          The code below will:

          1) Iterate through all all records sent by CloudWatch and process the DATA_MESSAGE type.
          2) Decode & Unzip the message payload
          3) Look for JSON payloads and load objects if found. 
          4) Package each log event message as a separate record, adding metadata and converting timestamp to utc date/time value
          5) combine all log event records into JSON array 
          6) Build KDF JSON response object with log events base64 encoded (NOTE: code will not GZIP output)

          The output to S3 file will:

          1) Be formatted as a JSON file that constitutes a JSON array with each log entry as a separate JSON object
          2) Each log entry will include metadata that can be omitted by OSI 

          Example output file:
          [
              {
                  "cloudwatch": {
                      "logGroup": "/aws/lambda/my-lambda-function",
                      "logStream": "2023/09/14/[$LATEST]9ae39e4917c04e7486a6cb81f892f33b",
                      "owner": "12334567890"
                  },
                  "id": "37793152654191904442680491486618333209652445892414210048",
                  "message": "[INFO] 2023-09-14T15:06:10.876Z 39b7ff85-5079-42bd-ae5e-39d102773384 example application message",
                  "timestamp": "2023-09-14T14:59:36.842000Z"
              },...
          ]


          Note: modify transformLogEvent() to change this output format to your desired output.

          """
          import boto3
          import base64
          import json
          import gzip
          from datetime import datetime
          import time
          import uuid
          import os
          import logging


          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          _LIST_KEY_NAME_ = "multivalue"
          _logGroup = ""
          _logStream = ""
          _owner = ""
          _cloudwatch_metadata = {}


          def has_key(thedict, keyvalue):
              if thedict.get(keyvalue) == None:
                return False
              else:
                return True


          def transformLogEvent(log_event):
              """Transform each log event.

              The default implementation below just extracts the message and appends a newline to it.

              Args:
              log_event (dict): The original log event. Structure is {"id": str, "timestamp": long, "message": str}

              Returns:
              str: The transformed log event.
              """
              processed_log_event = {
                  "message": "",
                  "timestamp": "",
                  "id": ""
              }
                      
              # reformat timestamp
              epoch_time_datetime = datetime.fromtimestamp(log_event['timestamp']/1000).isoformat()+'Z'
              
              logger.info("[KDFXFORM] processing record: "+str(log_event))
              
              try:
                  
                  # try to parse message for json 
                  json_message = json.loads(log_event["message"])
                  logger.info("[KDFXFORM] JSON Object Found! Using JSON object as log event. message set to JSON string value.")
                  
                  # if no exception thrown, message is json.  use dict object a log event message.
                  # if object is an array then it must be enclosed in a JSON dict object
                  if isinstance(json_message, list):
                      logger.info("[KDFXFORM] JSON object type is <list>.  Assigning to global key name <{}>".format(_LIST_KEY_NAME_))
                      processed_log_event[_LIST_KEY_NAME_] = json_message.copy()
                  else:
                      logger.info("[KDFXFORM] JSON object type is {}. Using json object direct copy".format(str(type(json_message))))
                      processed_log_event = json_message.copy()

                  processed_log_event['json_object'] = str(type(json_message))
                  processed_log_event['message'] = log_event["message"]
                  processed_log_event['timestamp'] = epoch_time_datetime
                  processed_log_event['id'] = log_event['id']   # uncomment if you want to keep the original id

                
                  
              except:
                  if type(log_event) is dict: 

                      if has_key(log_event,"message"): 
                          processed_log_event['message'] = log_event["message"]
                      else:
                          logger.error("[KDFXFORM] \"message\" key not found in log event!  setting to null.")
                          processed_log_event['message'] = ""
                          

                      processed_log_event['timestamp'] = epoch_time_datetime
                      
                      if has_key(log_event,"id"): 
                          processed_log_event['id'] = log_event["id"]   # uncomment if you want to keep the original id
                      else:
                          logger.error("[KDFXFORM] \"id\" key not found in log event!  setting to null.")
                          processed_log_event['id'] = ""               
                  else:
                      # json parsing failure means message is string.  use original string as log event message.
                      logger.error("[KDFXFORM] log event is not a dict! Skipped Processing.")
                      processed_log_event = log_event
              
              
              return processed_log_event





          def processRecords(records):
              """Process the records.
              
              This function processes the records and returns the processed records.
              
              Args:
                  records (list): The list of records to process.
              
              Returns:
                  list: A list of processed records.
              
              """

              processedRecords=[]
              result = []
              
              for record in records:
                  # Kinesis data streams are base64 encoded so decode here
                  payload = loadJsonGzipBase64(record['data'])

                  # set cloudwatch met values
                  try:
                      _logGroup = payload['logGroup']
                      _logStream = payload['logStream']
                      _owner = payload['owner']
                      _cloudwatch_metadata = {
                          "logGroup": _logGroup,
                          "logStream": _logStream,
                          "owner": _owner
                      }
                      
                      logger.info("[KDFXFORM] processing next record with cloudwatch metadata:" + str(_cloudwatch_metadata))

                      
                  except Exception as ex:
                      # log error message
                      logger.error("[KDFXFORM] ERROR trying to access logevent metadata values: "+str(ex))
                      
                  # process the record

                  if(payload['messageType'] == 'DATA_MESSAGE'):

                      for log_event in payload['logEvents']:
                          # append to list of processed records
                          xform_event = transformLogEvent(log_event)
                          
                          xform_event["cloudwatch"] = {
                              "logGroup": _logGroup,
                              "logStream": _logStream,
                              "owner": _owner
                          }
              
                          # log transformed event 
                          logger.info("[KDFXFORM] transformed event: "+str(xform_event))
                          
                          result.append(xform_event)
                  

              if(len(result)>0):
                      b64result = base64.b64encode(json.dumps(result).encode("utf-8"))
                      processedRecords.append(
                              {
                                  'recordId': record['recordId'],
                                  'result': 'Ok',
                                  'data': b64result
                              }
                      )
                          
              else:
                      processedRecords.append(
                              {
                                  'recordId': record['recordId'],
                                  'result': 'Dropped',
                                  'data': record['data']
                              }
                      )


              # return list of processed records
              return processedRecords



          def loadJsonGzipBase64(base64Data):
              return json.loads(gzip.decompress(base64.b64decode(base64Data)))



          def lambda_handler(event, context):
              

              """
              This function receives the event from Kinesis Firehose and processes the records.
              """
              # process the records
              records = processRecords(event['records'])
              # print the records
              x = { "records": records }
              logger.info("FINAL VALUE: "+str(x))

              # return the processed records
              return {'records': records}


Outputs:
  BucketName:
    Value: !Ref S3BucketLogs
    Description: Name of the S3 bucket containing the cloudwatch logs

  KinesisFirehoseDeliveryStream:
    Value: !GetAtt KinesisFirehoseDeliveryStream.Arn
    Description: Kinesis Firehose Delivery Stream Arn

  CloudWatchLogsToKinesisRoleArn:
    Value: !GetAtt IAMRoleCloudWatchLogsToKinesisRole.Arn
    Description: CloudWatch Logs to Kinesis Role Arn 

  OSSCollection:
    Value: !GetAtt OSSCollection.CollectionEndpoint
    Description: OpenSearch Serverless Collection Endpoint 


